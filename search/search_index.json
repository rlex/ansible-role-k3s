{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Ansible role for managing rancher k3s, lightweight, cncf-certified kubernetes distribution.</p>"},{"location":"advanced-configuration/additional-k8s-configs/","title":"Creating additional kubernetes configs","text":"<p>Sometimes you need to create additional config files for adding to kubernetes installation. For example, you want to trace kubelet, which requires separate config file for tracing configuration. Variable <code>k3s_extra_config_files</code> will take care of that. All additional config files will go to <code>/etc/rancher/k3s</code> directory, with name specified in name block and with content specified in content. This action will happen on pre-configuration stage, before k3s installation.  </p> <p>Example: <pre><code>k3s_extra_config_files:\n- name: apiserver-tracing.yaml\ncontent: |\napiVersion: apiserver.config.k8s.io/v1alpha1\nkind: TracingConfiguration\nendpoint: 127.0.0.1:4317\nsamplingRatePerMillion: 100\n</code></pre></p> <p>Will result in file <code>/etc/rancher/k3s/apiserver-tracing.yaml</code> with following content: <pre><code>apiVersion: apiserver.config.k8s.io/v1alpha1\nkind: TracingConfiguration\nendpoint: 127.0.0.1:4317\nsamplingRatePerMillion: 100\n</code></pre></p> <p>Please note that no additional formatting or processing is happening on that stage, so you need to take care about all indentation and other formatting things. Additionally, editing any of those files will trigger k3s restart</p>"},{"location":"advanced-configuration/additional-packages-and-services/","title":"Additional packages and services","text":"<p>Sometimes certain software requires certain packages installed on host system. Some of examples are distributed filesystems like longhorn and openebs which require iscsid. While it's better to manage such software with dedicated roles, i included that variables for simplicity. If you want openebs-jiva or longhorn to work, you can specify <pre><code>k3s_extra_packages:\n- open-iscsi\nk3s_extra_services:\n- iscsid\n</code></pre> open-iscsi will be installed and iscsid service will be both started and enabled at boot-time before k3s installation</p>"},{"location":"advanced-configuration/containerd-template/","title":"Customizing containerd config template","text":"<p>If you use different version of k3s and/or you want to customize containerd template, you can override path to containerd template with <code>k3s_containerd_template</code> variable, for example:  <pre><code>k3s_containerd_template: \"{{ inventory_dir }}/files/k3s/containerd.toml.tmpl.j2\"\n</code></pre> In that case, role will look for containerd template in <code>files/k3s/containerd.toml.tmpl.j2</code> inside your inventory folder you defined in <code>ansible.cfg</code> </p>"},{"location":"advanced-configuration/custom-cni/","title":"Using custom network plugin","text":"<p>If you want to use something different and self-managed than default flannel you can set flannel backend to none, which will remove flannel completely: <pre><code>k3s_flannel_backend: none\n</code></pre> Additionally, if you want to use something with eBPF dataplane enabled (calico, cilium) you might need to disable kube-proxy and mount bpffs filesystem on host node: <pre><code>k3s_bpffs: true\nk3s_master_extra_config:\ndisable-kube-proxy: true\n</code></pre></p>"},{"location":"advanced-configuration/custom-manifests/","title":"Adding custom kubernetes manifests","text":"<p>If you need to create additional kubernetes objects after cluster creation, you can use k3s_extra_manifests variable.  Example with all possible parameters:</p> <pre><code>k3s_extra_manifests:\n- name: kata\nstate: present\ndefinition:\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\nname: kata\nhandler: kata\n</code></pre> <p>You can supply full definition in \"definition\" block, including resource name in metadata.name (kata in example). If your object doesn't contain metadata.name, then name from ansible will be used (kata in example). Object name in .definition have precedence and will be used if both .name and .definition.metadata.name exists. You can also control control resource state with state parameter (<code>absent</code>, <code>present</code>), which is set to <code>present</code> by default.  Object creation will be delegated to first node in your <code>k3s_master_group</code>, in case of multi-master setup it will be your \"initial\" master node. For RBAC, it will use k3s-generated <code>/etc/rancher/k3s/k3s.yaml</code> kubeconfig on same master server, which have cluster-admin rights.</p>"},{"location":"advanced-configuration/custom-registries/","title":"Adding custom registries","text":"<p>By using k3s_registries variable you can configure custom registries, both origins and mirrors. Format follows official config format. Example: <pre><code>k3s_registries:\nmirrors:\ndocker.io:\nendpoint:\n- \"https://mycustomreg.com:5000\"\nconfigs:\n\"mycustomreg:5000\":\nauth:\nusername: xxxxxx # this is the registry username\npassword: xxxxxx # this is the registry password\ntls:\ncert_file: # path to the cert file used in the registry\nkey_file:  # path to the key file used in the registry\nca_file:   # path to the ca file used in the registry\n</code></pre></p>"},{"location":"advanced-configuration/external-cloud-controller/","title":"Provisioning cluster using external cloud-controller-manager","text":"<p>By default, cluster will be installed with k3s \"dummy\" cloud controller manager. If you deploy your k3s cluster on supported cloud platform (for example hetzner with their ccm) you will need to specify following parameters before first cluster start, since cloud controller can't be changed after cluster deployment:</p> <pre><code>k3s_master_extra_config:\ndisable-cloud-controller: true\nk3s_kubelet_extra_config:\n- \"cloud-provider=external\"\n</code></pre>"},{"location":"advanced-configuration/getting-kubeconfig/","title":"Getting kubeconfig via role","text":"<p>Role have ability to download kubeconfig file to machine from where ansible was run. To use it, set following variables: <pre><code>k3s_kubeconfig: true\nk3s_kubeconfig_context: k3s-de1\n</code></pre> Role will perform following:</p> <ol> <li> <p>Copy <code>/etc/rancher/k3s/k3s.yml</code> to <code>~/.kube/config-${ k3s_kubeconfig_context</code> }</p> </li> <li> <p>Patch it with your preferred context name specified in <code>k3s_kubeconfig_context</code> variable instead of stock <code>default</code></p> </li> <li> <p>Patch it with proper server URL (by default, it will be ansible_host of first master node in group specified in variable <code>k3s_master_group</code>, with port 6443, aka \"initial master\"), but you can override it with <code>k3s_kubeconfig_server</code></p> </li> <li> <p>Download resulting file to machine running ansible with path <code>~/.kube/config-${ k3s_kubeconfig_context }</code>, in current example it will be <code>~/.kube/config-k3s-de1</code></p> </li> </ol> <p>And you can start using it right away. However, if your master is configured differently (HA IP, Load balancer, etc), you might want to specify server manually. For this, you can use <code>k3s_kubeconfig_server</code> variable: <pre><code>k3s_kubeconfig_server: \"master-ha.k8s.example.org:6443\"\n</code></pre> Please note that role will not track changes of <code>/etc/rancher/k3s/k3s.yml</code> - if you redeploy your k3s cluster and need new kubeconfig, just delete existing local kubeconfig to get new one.</p>"},{"location":"advanced-configuration/setting-kubelet-arguments/","title":"Setting kubelet arguments","text":"<p>To pass arguments for kubelet, you can use <code>k3s_kubelet_extra_config</code> variable: <pre><code>k3s_kubelet_extra_config:\n- \"image-gc-high-threshold=40\"\n- \"image-gc-low-threshold=30\"\n</code></pre></p>"},{"location":"advanced-configuration/setting-sysctl/","title":"Setting sysctl","text":""},{"location":"advanced-configuration/setting-sysctl/#setting-sysctl","title":"Setting sysctl","text":"<p>Role also allows setting arbitrary sysctl settings using <code>k3s_sysctl_config</code> variable in dict format: <pre><code>k3s_sysctl_configs:\nfs.inotify.max_user_instances: 128\n</code></pre> Settings defined with that varible will be persisted in <code>/etc/sysctl.d/99-k3s.conf</code> file, loading them after system reboots</p>"},{"location":"advanced-configuration/specifying-ip/","title":"k3s and external ip","text":"<p>Sometimes k3s fails to properly detect external and internal ip. For those, you can use <code>k3s_external_ip</code> and <code>k3s_internal_ip</code> variables, for example: Ie: <pre><code>k3s_external_ip: \"{{ ansible_default_ipv4['address'] }}\"\nk3s_internal_ip: \"{{ ansible_vpn0.ipv4.address }}\"\n</code></pre> In which case external ip will be ansible default ip and node ip (internal-ip) will be ip address of vpn0 interface</p>"},{"location":"gvisor/settings/","title":"Additional configuration","text":"<p>Role supports passing additional settings for gvisor using <code>k3s_gvisor_config</code>. For example, to enable host networking, use: <pre><code>k3s_gvisor_config:\nnetwork: host\n</code></pre> Which will become <pre><code>[runsc_config]\nnetwork = \"host\"\n</code></pre> in gvisor config</p>"},{"location":"gvisor/usage/","title":"Installation and usage","text":"<p>By setting k3s_gvisor to true role will install gvisor - google's application kernel for container.  By default it will use systrap mode, to switch it to kvm set k3s_gvisor_platform to kvm. If platform will be set to kvm, role will also load (and persist) corresponding module into kernel. It will also create RuntimeClass kubernetes object if you have variable k3s_gvisor_create_runtimeclass set to true (default). If you want to create it manually: <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\nname: gvisor\nhandler: runsc\n</code></pre> After that you should be able to launch gvisor-enabled pods by adding runtimeClassName to pod spec, ie <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: gvisor-nginx\nspec:\nruntimeClassName: gvisor\ncontainers:\n- name: nginx\nimage: nginx\n</code></pre></p>"},{"location":"installation/airgapped-install/","title":"Airgapped installation","text":"<p>For environments without internet access, you can use <pre><code>k3s_install_mode: airgap\n</code></pre> In that mode, role will download k3s binary and bootstrap images locally, and transfer them to target server from ansible runner. It will also work for gvisor. Please note that if you use additional manifests installation, you will need python3-kubernetes package installed on system - role assumes you have accessible OS distribution mirror configured on that airgapped node, otherwise installation will fail. If you can't get that package installed on your system, do not use automatic installation of manifests and set <pre><code>k3s_gvisor_create_runtimeclass: false\n</code></pre></p>"},{"location":"installation/basics/","title":"Basic installation","text":"<p>This role discovers installation mode from your ansible inventory.  For working with your inventory, it operates on two basic variables,  <code>k3s_master_group</code> and <code>k3s_agent_group</code>, which are set to <code>k3s_master</code> and <code>k3s_agent</code> by default.</p> <p>Following is an example of single master and 2 agents: <pre><code>[k3s_master]\nkube-master-1.example.org\n[k3s_agent]\nkube-node-1.example.org\nkube-node-2.example.org\n</code></pre></p> <p>For group with master, k3s_master in that example, you should enable master installation with <code>k3s_master</code> variable: <pre><code>k3s_master: true\n</code></pre></p> <p>Accordingly, for agents, use <code>k3s_agent</code> variable: <pre><code>k3s_agent: true\n</code></pre></p> <p>For selecting master server to connect, you can use <code>k3s_master_ip</code> variable. By default it will be set to first ansible_host in ansible group specified in <code>k3s_master_group</code> variable. Of course, you can always redefine it manually.</p>"},{"location":"installation/requirements/","title":"Requirements","text":"<p>Apart from what k3s requires, this role also needs systemd, so it should work on any modern distro.  </p>"},{"location":"installation/multi-master/install/","title":"Install","text":""},{"location":"installation/multi-master/install/#multi-master-setup","title":"Multi-master setup","text":"<p>When your <code>k3s_master</code> ansible inventory group have more than 1 host, role will detect it and switch to multi-master installation. First node in group will be used as \"bootstrap\", while following will bootstrap from first node. Any number of nodes is ok, but it's generally recommended to have odd number of nodes for etcd eletction to work, since etcd quorum is (n/2)+1 where n is number of nodes.  </p> <p>You can also switch existing, single-node sqlite master to multimaster configuration by adding more masters to existing install - be aware, however, that migration from single-node sqlite to etcd is supported only in k3s &gt;= 1.22!</p> <p>Pay attention that in default configuration all agents will be pointing only to first master, which is not really useful for HA setup. Configuring HA is out of scope for this role, so take a look at following docs:</p> <ol> <li>HA with haproxy</li> <li>HA with keepalived</li> </ol>"},{"location":"installation/multi-master/high-availability/haproxy/","title":"HA with haproxy","text":"<p>Using This haproxy role. I run my cluster on top of L3 vpn so i can't use L2, so i just install haproxy on each node, point haproxy to all masters, and point agents to localhost haproxy. Dirty, but works. Example config:</p> <pre><code>haproxy_listen:\n- name: stats\ndescription: Global statistics\nbind:\n- listen: '0.0.0.0:1936'\nmode: http\nhttp_request:\n- action: use-service\nparam: prometheus-exporter\ncond: if { path /metrics }\nstats:\nenable: true\nuri: /\noptions:\n- hide-version\n- show-node\nadmin: if LOCALHOST\nrefresh: 5s\nauth:\n- user: admin\npasswd: 'yoursupersecretpassword'\nhaproxy_frontend:\n- name: kubernetes_master_kube_api\ndescription: frontend with k8s api masters\nbind:\n- listen: \"127.0.0.1:16443\"\nmode: tcp\ndefault_backend: k8s-de1-kube-api\nhaproxy_backend:\n- name: k8s-de1-kube-api\ndescription: backend with all kubernetes masters\nmode: tcp\nbalance: roundrobin\noption:\n- httpchk GET /readyz\nhttp_check: expect status 401\ndefault_server_params:\n- inter 1000\n- rise 2\n- fall 2\nserver:\n- name: k8s-de1-master-1\nlisten: \"master-1:6443\"\nparam:\n- check\n- check-ssl\n- verify none\n- name: k8s-de1-master-2\nlisten: \"master-2:6443\"\nparam:\n- check\n- check-ssl\n- verify none\n- name: k8s-de1-master-3\nlisten: \"master-3:6443\"\nparam:\n- check\n- check-ssl\n- verify none\n</code></pre> <p>That will start haproxy listening on 127.0.0.1:16443 for connections to k8s masters. You can then redefine master IP and port for agents with <pre><code>k3s_master_ip: 127.0.0.1\nk3s_master_port: 16443\n</code></pre></p> <p>And now your connections are balanced between masters and protected in case of one or two masters will go down. One downside of that config is that it checks for reply 401 on /readyz endpoint, because since certain version of k8s (1.19 if i recall correctly) this endpoint requires authorization. So you have 2 options here:</p> <ul> <li>Continue to rely on 401 check (not a good solution, since we're just checking for http up status)</li> <li>Add <code>anonymous-auth=true</code> to apiserver arguments:        <pre><code>  k3s_master_extra_config:\nkube-apiserver-arg:\n- \"anonymous-auth=true\"\n</code></pre>     This will open /readyz, /healthz, /livez and /version endpoints to anonymous auth, and potentially expose version info. If that is concerning you, it's possible to patch system:public-info-viewer role to keep only /readyz, /healthz and /livez endpoint open:     <pre><code>kubectl patch clusterrole system:public-info-viewer --type=json -p='[{\"op\": \"replace\", \"path\": \"/rules/0/nonResourceURLs\", \"value\":[\"/healthz\",\"/livez\",\"/readyz\"]}]'\n</code></pre></li> </ul> <p>This proxy also works with initial agent join, so it's better to setup haproxy before installing k3s and then switching to HA config. It will also expose prometheus metrics on 0.0.0.0:1936/metrics - pay attention that this part (unlike webui) won't be protected by user and password, so adjust your firewall accordingly if needed!</p> <p>Of course you can use whatever you want - external cloud LB, nginx, anything, all it needs is TCP protocol support (because in this case we don't want to manage SSL on loadbalancer side). But haproxy provides you with prometheus metrics, have nice webui for monitoring and management, and i'm just familiar with it.</p>"},{"location":"installation/multi-master/high-availability/keepalived/","title":"HA with VRRP (keepalived)","text":"<p>You can use this keepalived role if you have L2 networking available and can use VRRP for failover IP. In that case, you might need to add tls-san option in k3s_master_extra_config with your floating ip. For keepalived to work, following conditions should be met:   1) L2 networking must be available. Sadly, this is not a common case with cloud providers and most VPNs.   2) Virtual IP must be in same subnet as interfaces on top of which they are used</p> <p>Sample keepalived configuration on master-1, assuming we use network 10.91.91.0/24 on vpn0 interface: <pre><code>keepalived_instances:\nvpn:\ninterface: vpn0\nstate: MASTER\nvirtual_router_id: 51 #if you have multiple VRRP setups in same network this should be unique\npriority: 255 #node usually owning IP should always have priority set to 255\nauthentication_password: \"somepassword\" #can be omitted, but always good to use\nvips:\n- \"10.91.91.50 dev vpn0 label vpn0:0\"\n</code></pre> for backing masters: <pre><code>keepalived_instances:\nvpn:\ninterface: vpn0\nstate: BACKUP\nvirtual_router_id: 51\npriority: 254 #use lower priority for each node\nauthentication_password: \"somepassword\"\nvips:\n- \"10.91.91.50 dev vpn0 label vpn0:0\"\n</code></pre> And in k3s configuration: <pre><code>k3s_master_extra_config:\ntls-san: 10.91.91.50\n</code></pre></p> <p>If everything is configured correctly, you should see 10.91.91.50 on vpn0:0 interface on master-1 node. Try stopping keepalived on master-1 and see how IP disappears from master-1 and appears on master-2. From now it's your choice how you want to configure HA - point agents to that floating IP, or install load-balancer on each master node and distribute requests between them.</p>"}]}